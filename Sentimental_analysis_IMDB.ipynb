{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentimental analysis on IMDB movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are two main approaches to sentiment analysis using a lexicon of pre-recorded sentiment or using state of the art but more computationally expensive deep learning to learn generalized vector representation from words Feeforward net accepts fixed sized inputs like binary numbers but recurrent neural nets helps us learn from sequences data, like texts and you can use AWS with pre-build AMI (Bitfusion to host jupyter notebook on cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tflearn\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "from tflearn.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMDB Dataset loading\n",
    "train,test, _ = imdb.load_data(path='imdb.pkl',n_words=10000,\n",
    "                              valid_portion=0.1)\n",
    "trainX, trainY = train\n",
    "testX, testY = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing \n",
    "# Sequence Padding \n",
    "# we can't just feed text strings into a neural network directly,\n",
    "# we have to vectorize our inputs\n",
    "# neural network are algorithms that essentially\n",
    "# just apply a series of computations to your matrices.\n",
    "# so converting them to numerical representations \n",
    "# or vectors is necessary\n",
    "\n",
    "#convert each review into a matrix and pad it(pad_sequences)\n",
    "#sequence padding padding value 0\n",
    "trainX = pad_sequences(trainX,maxlen=100,value=0.)\n",
    "testX  = pad_sequences(testX, maxlen=100,value=0.)\n",
    "\n",
    "#Converting labels to binary vectors\n",
    "trainY = to_categorical(trainY,nb_classes=2)\n",
    "testY  = to_categorical(testY,nb_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM: This layer allows out network to remember data from begining of the sequences which will improve our prediction\n",
    "### We will set dropout to .08 which is a technique that helps prevent overfitting by randomly turning on and off different pathways in our network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our next layer is fully connected which means that every neuron in the previous layer is connected to every neuron in this layer.\n",
    "### we have a set of learned feature vectors from previous layers, and adding a fully connected layer is a computationally cheap way of learning non-linear combinations of them\n",
    "### Its got 2 units and it's using the softmax function as its activation function this will take in a vector of values and squash it into vector of output probabilities between 0 and 1, that sums up to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We'll use those values in out last layer, which is our regression layer. this will apply a regression operation to the input We're going to specify an optimizer method that will minimize a given loss function as well as the learning rate, which specifies how fast we want our network to train. The optimizer we'll use is adam, which performs gradient descent and categorical cross entropy is our loss, it helps to find the difference between our predicted output and the expected output After building our neural network we can go ahead and initialize it using tflearn's deep neural net function then we can call our models fit function which will launch the training process for given training and validation data we'll also set show metric to true so we can view the log of accuracy during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network building\n",
    "#input layer\n",
    "net = tflearn.input_data([None,100]) # param: [batch_size,length]\n",
    "# Embedding Layer\n",
    "net = tflearn.embedding(net,input_dim=10000,output_dim=128) # param :[output of previous layer ,input dimension ,next layer reduced dimension]\n",
    "#lstm layer (long short term memory)\n",
    "net = tflearn.lstm(net,128,dropout=0.8)\n",
    "# fully connected layer\n",
    "net = tflearn.fully_connected(net,2,activation='softmax')\n",
    "#activation layer\n",
    "net = tflearn.regression(net,optimizer ='adam',learning_rate=0.001,\n",
    "                        loss='categorical_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 5979  | total loss: \u001b[1m\u001b[32m0.08378\u001b[0m\u001b[0m | time: 68.736s\n",
      "\u001b[2K\r",
      "| Adam | epoch: 009 | loss: 0.08378 - acc: 0.9682 -- iter: 11104/22500\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "# DNN is Deep Neural Net function of tflearn\n",
    "model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "model.fit(trainX, trainY, validation_set=(testX, testY), show_metric=True,\n",
    "          batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
